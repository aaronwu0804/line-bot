#!/usr/bin/env python3
"""
é€£çµåˆ†æå·¥å…·æ¨¡çµ„
ä½¿ç”¨ Gemini API çš„ URL Context åŠŸèƒ½åˆ†æç¶²é é€£çµ
æ”¯æ´ç¶²é å…§å®¹æå–ã€æ‘˜è¦å’Œ Google Search Grounding
"""

import os
import json
import logging
import re
from typing import Dict, Optional, List

logger = logging.getLogger(__name__)

# å°å…¥ Gemini API
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logger.warning("Gemini API æœªå®‰è£ï¼Œé€£çµåˆ†æåŠŸèƒ½å°‡å—é™")


class LinkAnalyzer:
    """é€£çµåˆ†æå™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–é€£çµåˆ†æå™¨
        
        Args:
            api_key: Gemini API é‡‘é‘°
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        self.model = None
        self.enabled = False
        
        if self.api_key and GEMINI_AVAILABLE:
            try:
                genai.configure(api_key=self.api_key)
                
                # ä½¿ç”¨æ”¯æ´ URL Context çš„æ¨¡å‹
                self.model = genai.GenerativeModel(
                    'gemini-2.0-flash-exp',
                    tools=[
                        {'google_search': {}},  # å•Ÿç”¨ Google æœå°‹
                        {
                            'url_context': {
                                'max_urls': 5,
                                'supported_mime_types': ['text/html', 'application/pdf']
                            }
                        }
                    ]
                )
                self.enabled = True
                logger.info("é€£çµåˆ†æå™¨å·²ä½¿ç”¨ Gemini API + URL Context åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ– Gemini API å¤±æ•—: {e}")
                self.enabled = False
        else:
            logger.warning("Gemini API æœªè¨­å®šæˆ–ä¸å¯ç”¨ï¼Œé€£çµåˆ†æåŠŸèƒ½å°‡å—é™")
    
    def extract_urls(self, text: str) -> List[str]:
        """
        å¾æ–‡å­—ä¸­æå– URL
        
        Args:
            text: æ–‡å­—å…§å®¹
            
        Returns:
            List[str]: URL åˆ—è¡¨
        """
        # URL æ­£å‰‡è¡¨é”å¼
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        urls = re.findall(url_pattern, text)
        return urls
    
    async def analyze_link(self, url: str, user_query: Optional[str] = None) -> Dict:
        """
        åˆ†æé€£çµå…§å®¹
        
        Args:
            url: è¦åˆ†æçš„ç¶²å€
            user_query: ç”¨æˆ¶çš„æŸ¥è©¢æˆ–å•é¡Œï¼ˆå¯é¸ï¼‰
            
        Returns:
            Dict: åˆ†æçµæœ
        """
        if not self.enabled:
            return {
                "success": False,
                "error": "é€£çµåˆ†æåŠŸèƒ½æœªå•Ÿç”¨",
                "url": url
            }
        
        try:
            # æ§‹å»ºæç¤ºè©
            if user_query:
                prompt = f"""è«‹åˆ†æä»¥ä¸‹ç¶²å€çš„å…§å®¹ï¼Œä¸¦æ ¹æ“šç”¨æˆ¶çš„å•é¡Œæä¾›å›ç­”ï¼š

ç¶²å€ï¼š{url}

ç”¨æˆ¶å•é¡Œï¼š{user_query}

è«‹æä¾›ï¼š
1. ç¶²é çš„ä¸»è¦å…§å®¹æ‘˜è¦
2. é‡å°ç”¨æˆ¶å•é¡Œçš„å…·é«”å›ç­”
3. ç›¸é—œçš„é‡é»è³‡è¨Š

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
            else:
                prompt = f"""è«‹åˆ†æä»¥ä¸‹ç¶²å€çš„å…§å®¹ï¼š

ç¶²å€ï¼š{url}

è«‹æä¾›ï¼š
1. ç¶²é æ¨™é¡Œ
2. ä¸»è¦å…§å®¹æ‘˜è¦ï¼ˆ200å­—ä»¥å…§ï¼‰
3. é—œéµè¦é»ï¼ˆ3-5é»ï¼‰
4. å…§å®¹é¡å‹ï¼ˆä¾‹å¦‚ï¼šæ–°èã€æ•™å­¸ã€éƒ¨è½æ ¼æ–‡ç« ç­‰ï¼‰

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
            
            # ä½¿ç”¨ Gemini API åˆ†æ
            response = self.model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=1.0  # ä½¿ç”¨å»ºè­°çš„æº«åº¦è¨­å®š
                )
            )
            
            result_text = response.text.strip()
            
            logger.info(f"é€£çµåˆ†æå®Œæˆ: url={url}")
            
            return {
                "success": True,
                "url": url,
                "analysis": result_text,
                "query": user_query
            }
        
        except Exception as e:
            logger.error(f"åˆ†æé€£çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": url
            }
    
    async def analyze_multiple_links(self, urls: List[str], user_query: Optional[str] = None) -> Dict:
        """
        åˆ†æå¤šå€‹é€£çµ
        
        Args:
            urls: URL åˆ—è¡¨
            user_query: ç”¨æˆ¶çš„æŸ¥è©¢æˆ–å•é¡Œï¼ˆå¯é¸ï¼‰
            
        Returns:
            Dict: åˆ†æçµæœ
        """
        if not self.enabled:
            return {
                "success": False,
                "error": "é€£çµåˆ†æåŠŸèƒ½æœªå•Ÿç”¨",
                "urls": urls
            }
        
        # é™åˆ¶è™•ç†çš„é€£çµæ•¸é‡
        urls = urls[:5]  # æœ€å¤šè™•ç† 5 å€‹é€£çµ
        
        try:
            # æ§‹å»ºæç¤ºè©
            urls_text = "\n".join([f"{i+1}. {url}" for i, url in enumerate(urls)])
            
            if user_query:
                prompt = f"""è«‹åˆ†æä»¥ä¸‹ç¶²å€çš„å…§å®¹ï¼Œä¸¦æ ¹æ“šç”¨æˆ¶çš„å•é¡Œæä¾›ç¶œåˆå›ç­”ï¼š

ç¶²å€åˆ—è¡¨ï¼š
{urls_text}

ç”¨æˆ¶å•é¡Œï¼š{user_query}

è«‹æä¾›ï¼š
1. å„ç¶²é çš„ç°¡è¦æ‘˜è¦
2. é‡å°ç”¨æˆ¶å•é¡Œçš„ç¶œåˆå›ç­”
3. ä¾†è‡ªä¸åŒä¾†æºçš„ç›¸é—œè³‡è¨Šæ¯”è¼ƒ

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
            else:
                prompt = f"""è«‹åˆ†æä»¥ä¸‹ç¶²å€çš„å…§å®¹ä¸¦æä¾›ç¶œåˆæ‘˜è¦ï¼š

ç¶²å€åˆ—è¡¨ï¼š
{urls_text}

è«‹æä¾›ï¼š
1. å„ç¶²é çš„ä¸»è¦å…§å®¹æ‘˜è¦
2. å…±åŒä¸»é¡Œæˆ–é—œè¯æ€§
3. å€¼å¾—æ³¨æ„çš„é‡é»è³‡è¨Š

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
            
            # ä½¿ç”¨ Gemini API åˆ†æ
            response = self.model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=1.0
                )
            )
            
            result_text = response.text.strip()
            
            logger.info(f"å¤šé€£çµåˆ†æå®Œæˆ: count={len(urls)}")
            
            return {
                "success": True,
                "urls": urls,
                "analysis": result_text,
                "query": user_query
            }
        
        except Exception as e:
            logger.error(f"åˆ†æå¤šå€‹é€£çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "success": False,
                "error": str(e),
                "urls": urls
            }
    
    def format_analysis_for_display(self, analysis_result: Dict) -> str:
        """
        æ ¼å¼åŒ–åˆ†æçµæœç‚ºé¡¯ç¤ºæ–‡å­—
        
        Args:
            analysis_result: åˆ†æçµæœ
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡å­—
        """
        if not analysis_result.get("success"):
            error = analysis_result.get("error", "æœªçŸ¥éŒ¯èª¤")
            return f"âŒ é€£çµåˆ†æå¤±æ•—ï¼š{error}"
        
        lines = ["ğŸ”— é€£çµåˆ†æçµæœï¼š\n"]
        
        # é¡¯ç¤ºåˆ†æçš„ç¶²å€
        if "urls" in analysis_result:
            lines.append("ğŸ“Œ åˆ†æçš„ç¶²å€ï¼š")
            for url in analysis_result["urls"]:
                lines.append(f"  â€¢ {url}")
            lines.append("")
        elif "url" in analysis_result:
            lines.append(f"ğŸ“Œ ç¶²å€ï¼š{analysis_result['url']}\n")
        
        # é¡¯ç¤ºåˆ†æå…§å®¹
        analysis = analysis_result.get("analysis", "")
        if analysis:
            lines.append(analysis)
        
        return "\n".join(lines)


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
link_analyzer = LinkAnalyzer()


# é€£çµå„²å­˜ç®¡ç†
class LinkStorage:
    """é€£çµå„²å­˜ç®¡ç†å™¨"""
    
    def __init__(self, storage_dir: str = ".cache/links"):
        """
        åˆå§‹åŒ–é€£çµå„²å­˜ç®¡ç†å™¨
        
        Args:
            storage_dir: é€£çµå„²å­˜ç›®éŒ„
        """
        self.storage_dir = storage_dir
        os.makedirs(storage_dir, exist_ok=True)
        logger.info(f"é€£çµå„²å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œå„²å­˜ç›®éŒ„: {storage_dir}")
    
    def _get_user_file(self, user_id: str) -> str:
        """ç²å–ç”¨æˆ¶é€£çµæª”æ¡ˆè·¯å¾‘"""
        return os.path.join(self.storage_dir, f"{user_id}_links.json")
    
    def _load_links(self, user_id: str) -> List[Dict]:
        """è¼‰å…¥ç”¨æˆ¶çš„é€£çµ"""
        file_path = self._get_user_file(user_id)
        
        if not os.path.exists(file_path):
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥é€£çµå¤±æ•—: {e}")
            return []
    
    def _save_links(self, user_id: str, links: List[Dict]) -> bool:
        """å„²å­˜ç”¨æˆ¶çš„é€£çµ"""
        file_path = self._get_user_file(user_id)
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(links, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            logger.error(f"å„²å­˜é€£çµå¤±æ•—: {e}")
            return False
    
    def save_link(self, user_id: str, url: str, title: Optional[str] = None,
                 summary: Optional[str] = None, tags: Optional[List[str]] = None) -> Dict:
        """
        å„²å­˜é€£çµ
        
        Args:
            user_id: ç”¨æˆ¶ ID
            url: ç¶²å€
            title: æ¨™é¡Œï¼ˆå¯é¸ï¼‰
            summary: æ‘˜è¦ï¼ˆå¯é¸ï¼‰
            tags: æ¨™ç±¤ï¼ˆå¯é¸ï¼‰
            
        Returns:
            Dict: å„²å­˜çµæœ
        """
        from datetime import datetime
        
        links = self._load_links(user_id)
        
        new_link = {
            "id": f"link_{len(links) + 1}_{datetime.now().timestamp()}",
            "url": url,
            "title": title or url,
            "summary": summary,
            "tags": tags or [],
            "saved_at": datetime.now().isoformat()
        }
        
        links.append(new_link)
        
        if self._save_links(user_id, links):
            logger.info(f"é€£çµå„²å­˜æˆåŠŸ: user_id={user_id}, url={url}")
            return {"success": True, "link": new_link}
        else:
            return {"success": False, "error": "å„²å­˜å¤±æ•—"}
    
    def query_links(self, user_id: str, keyword: Optional[str] = None, limit: int = 10) -> Dict:
        """
        æŸ¥è©¢é€£çµ
        
        Args:
            user_id: ç”¨æˆ¶ ID
            keyword: æœå°‹é—œéµå­—ï¼ˆå¯é¸ï¼‰
            limit: è¿”å›çµæœæ•¸é‡é™åˆ¶
            
        Returns:
            Dict: æŸ¥è©¢çµæœ
        """
        links = self._load_links(user_id)
        
        # éæ¿¾æ¢ä»¶
        if keyword:
            keyword_lower = keyword.lower()
            links = [
                link for link in links
                if keyword_lower in link.get("title", "").lower() or
                   keyword_lower in link.get("summary", "").lower() or
                   keyword_lower in link.get("url", "").lower() or
                   keyword_lower in " ".join(link.get("tags", [])).lower()
            ]
        
        # æŒ‰æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        links.sort(key=lambda x: x.get("saved_at", ""), reverse=True)
        
        # é™åˆ¶æ•¸é‡
        links = links[:limit]
        
        logger.info(f"é€£çµæŸ¥è©¢å®Œæˆ: user_id={user_id}, found={len(links)}")
        return {"success": True, "links": links, "count": len(links)}


# å»ºç«‹é€£çµå„²å­˜ç®¡ç†å™¨å¯¦ä¾‹
link_storage = LinkStorage()


# æ¸¬è©¦å‡½æ•¸
if __name__ == "__main__":
    import asyncio
    
    # æ¸¬è©¦é€£çµæå–
    test_text = "è«‹çœ‹é€™ç¯‡æ–‡ç«  https://example.com/article å’Œ https://test.com/page"
    urls = link_analyzer.extract_urls(test_text)
    print(f"æå–çš„ URL: {urls}")
    
    # æ¸¬è©¦é€£çµå„²å­˜
    test_user_id = "test_user_123"
    result = link_storage.save_link(
        test_user_id,
        "https://example.com/article",
        title="ç¯„ä¾‹æ–‡ç« ",
        summary="é€™æ˜¯ä¸€ç¯‡ç¯„ä¾‹æ–‡ç« çš„æ‘˜è¦",
        tags=["ç¯„ä¾‹", "æ¸¬è©¦"]
    )
    print(f"å„²å­˜é€£çµ: {result}")
    
    # æŸ¥è©¢é€£çµ
    result = link_storage.query_links(test_user_id)
    print(f"æŸ¥è©¢é€£çµ: {result}")
